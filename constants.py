experiments_mapping = { 
                        "Adam's ε": "epsilon",
                        "Batch Size": "batch_sizes",
                        "Conv. Activation Function": "layer_funct_conv",
                        "Convolutional Normalization": "normalizations_convs", 
                        "Convolutional Width": "CNN_widths",
                        "Dense Activation Function": "layer_funct_dense",
                        "Dense Normalization": "normalizations",
                        "Dense Width": "widths",
                        "Discount Factor": "gammas",
                        "Exploration ε": "eps_train",
                        "Learning Rate": "learning_rate",
                        "Minimum Replay History": "min_replay_history",
                        "Number of Atoms": "num_atoms", 
                        "Number of Convolutional Layers": "convs", 
                        "Number of Dense Layers": "depths",
                        "Replay Capacity": "replay_capacity",
                        "Reward Clipping": "clip_rewards",
                        "Target Update Period": "target_update_periods",
                        "Update Horizon": "update_horizon",
                        "Update Period": "update_periods",
                        "Weight Decay": "weightdecay",
                    }

ATARI_100K_GAMES = [
            'Alien', 'Amidar', 'Assault', 'Asterix', 'BankHeist', 'BattleZone',
            'Boxing', 'Breakout', 'ChopperCommand', 'CrazyClimber', 'DemonAttack',
            'Freeway', 'Frostbite', 'Gopher', 'Hero', 'Jamesbond', 'Kangaroo',
            'Krull', 'KungFuMaster', 'MsPacman', 'Pong', 'PrivateEye', 'Qbert',
            'RoadRunner', 'Seaquest', 'UpNDown'
            ]

THC_METRIC = {
    'DrQ_eps': 
               {"Adam's ε": (0.8333333333333334, 0.14433756729740646), 
                'Batch Size': (0.7083333333333334, 0.25),
                'Conv. Activation Function': (0.18333333333333335, 0.1329160135825126),
                'Convolutional Normalization': (0.8333333333333334, 0.14433756729740646),
                'Convolutional Width': (0.7, 0.14252192813739223),
                'Dense Activation Function': (0.5166666666666667, 0.09831920802501748),
                'Dense Width': (0.75, 0.09622504486493767),
                'Discount Factor': (0.8333333333333334, 0.14433756729740646),
                'Exploration ε': (0.4, 0.05590169943749474),
                'Learning Rate': (0.5, 0.19764235376052372),
                'Minimum Replay History': (0.3333333333333333, 0.0),
                'Number of Convolutional Layers': (0.75, 0.25),
                'Number of Dense Layers': (0.6666666666666666, 0.14433756729740646),
                'Replay Capacity': (0.5416666666666666, 0.08333333333333331),
                'Reward Clipping': (1.0, 0.0),
                'Target Update Period': (0.075, 0.06846531968814577),
                'Update Horizon': (0.775, 0.22360679774997896),
                'Update Period': (0.6666666666666666, 0.23570226039551584),
                'Weight Decay': (0.575, 0.11180339887498948)},
    'DER': 
            {"Adam's ε": (0.5833333333333334, 0.14433756729740646),
             'Batch Size': (0.6666666666666666, 0.23570226039551584),
             'Conv. Activation Function': (0.19999999999999998, 0.10954451150103323),
             'Convolutional Normalization': (0.5833333333333334, 0.14433756729740646),
             'Convolutional Width': (0.55, 0.2091650066335189),
             'Dense Activation Function': (0.4333333333333333, 0.0816496580927726),
             'Dense Width': (0.7083333333333334, 0.25),
             'Discount Factor': (0.9166666666666666, 0.14433756729740646),
             'Exploration ε': (0.175, 0.06846531968814576),
             'Learning Rate': (0.45, 0.2592055169165965),
             'Minimum Replay History': (0.20833333333333331, 0.08333333333333333),
             'Number of Atoms': (0.625, 0.28463752127665554),
             'Number of Convolutional Layers': (0.75, 0.25),
             'Number of Dense Layers': (0.5833333333333334, 0.14433756729740646),
             'Replay Capacity': (0.4583333333333333, 0.08333333333333334),
             'Reward Clipping': (1.0, 0.0),
             'Target Update Period': (0.25, 0.08838834764831845),
             'Update Horizon': (0.7, 0.14252192813739223),
             'Update Period': (0.5833333333333333, 0.09622504486493762),
             'Weight Decay': (0.7, 0.06846531968814577)}
    }


THC_METRIC_100k = {
    'DrQ_eps': {
        "Adam's ε": (0.39999999999999997, 0.15491933384829668),
        'Batch Size': (0.575, 0.22707377655731187),
        'Conv. Activation Function': (0.3166666666666667, 0.1329160135825126),
        'Convolutional Normalization': (0.4166666666666667, 0.14433756729740643),
        'Convolutional Width': (0.2, 0.16770509831248423),
        'Dense Activation Function': (0.4666666666666666, 0.12110601416389963),
        'Dense Normalization': (0.5833333333333334, 0.28867513459481287),
        'Dense Width': (0.15, 0.10458250331675945),
        'Discount Factor': (0.7916666666666667, 0.08333333333333337),
        'Exploration ε': (0.3, 0.14252192813739226),
        'Learning Rate': (0.5, 0.10758287072798381),
        'Minimum Replay History': (0.125, 0.125),
        'Number of Convolutional Layers': (0.5416666666666666, 0.08333333333333331),
        'Number of Dense Layers': (0.45833333333333326, 0.15957118462605635),
        'Replay Capacity': (0.4, 0.05590169943749474),
        'Reward Clipping': (1.0, 0.0),
        'Target Update Period': (0.3, 0.14252192813739226),
        'Update Horizon': (0.6833333333333332, 0.16020819787597224),
        'Update Period': (0.3833333333333333, 0.07527726527090811),
        'Weight Decay': (0.3666666666666667, 0.196638416050035)},
    'DER': {
        "Adam's ε": (0.4166666666666667, 0.2136976056643281),
        'Batch Size': (0.6, 0.24044230077089182),
        'Conv. Activation Function': (0.2333333333333333, 0.16329931618554522),
        'Convolutional Normalization': (0.6666666666666666, 0.28867513459481287),
        'Convolutional Width': (0.325, 0.14252192813739226),
        'Dense Activation Function': (0.5166666666666666, 0.14719601443879743),
        'Dense Normalization': (0.6666666666666666, 0.14433756729740646),
        'Dense Width': (0.375, 0.125),
        'Discount Factor': (0.8333333333333334, 0.13608276348795434),
        'Exploration ε': (0.1, 0.10458250331675945),
        'Learning Rate': (0.5595238095238095, 0.07926581093427848),
        'Minimum Replay History': (0.1, 0.055901699437494755),
        'Number of Atoms': (0.15, 0.05590169943749474),
        'Number of Convolutional Layers': (0.5833333333333333, 0.0962250448649376),
        'Number of Dense Layers': (0.5416666666666666, 0.08333333333333331),
        'Replay Capacity': (0.45, 0.06846531968814576),
        'Reward Clipping': (0.5, 0.0),
        'Target Update Period': (0.125, 0.0),
        'Update Horizon': (0.5166666666666667, 0.2041241452319315),
        'Update Period': (0.43333333333333335, 0.05163977794943222),
        'Weight Decay': (0.43333333333333335, 0.1505545305418162)}
}

aggregators = ["IQM", "Mean", "Median","Optimality Gap"]